# Attention 的深入理解

> 注意力就是一个加权和

$softmax(qK^{T})V$

> q 是行向量，$K^{T}$每一列表示一个词，V 每一行表示一个词
> 如果是$softmax(QK^{T})V$，那么softmax对应单词对每一行操作

如果是self Attention，即 q 来自 K 中的某个词，则得到 q 的新表达，这个表达等于 K 中所有词的加权和

如果是encoder-decoder Attention，即 q 不是 K 中的词，则得到 q 的下一个预测，这个预测等于 K 中所有词的加权和
