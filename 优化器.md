### 一、优化器

**1. 三种常见的优化器**

> 三种优化器的参数量是一样的，x~i~有多少个特征，输入就有多少个节点，只不过SGD是一次迭代后更新，而BGD是N次迭代的累加和后除以N再更新。
>
> > **那么问题来了，机器翻译中，所谓的输入一句话，是怎么实现的，Embedding Average?**
>
> x~i~只有一个特征的话，梯度只有左右两种方向。
> x~i~有两个特征的话，梯度有360°个方向。
> x~i~有三个特征的话，梯度有球个方向。



a. BGD（Batch Gradient Descent）批梯度下降
采用整个数据集计算 梯度，

![](BGD.png)

缺点：

- 更新慢，不能增加新数据实时更新数据
- 局部最优，鞍点

b. SGD（Stochastic Gradient Descent）随机梯度下降
每次采用一个数据计算梯度，

![](SGD.png)

优缺点：

- 训练快，但不会很快收敛
- 能增加新数据
- 比较震荡，因为并不是每次都向整体最优的方向更新，好处是可能因为震荡而重一个局部最优到另一个局部最优
- 同样存在局部最优和鞍点问题

c. MBGD（Min-Batch Gradient Descent）小批量梯度下降
每次去最小批更新数据，

![](MBGD.png)

缺点：

- 避开上述两者缺点，比BGD更新快，比SGD稳定。
- 充分利用gpu并行计算特点

**2. 挑战**

- learning rate 固定，小了训练慢，大了不收敛
- 局部最优和鞍点

**3. 自适应梯度下降法**

> 所谓自适应，是指自适应学习率
>
> 数据稀疏时用自适应算法好，因为0项多的话，用sgd求得的梯度可能是0

- Adam
- 动量

优点：

- 自适应收敛快，sgd收敛缓慢

参考：

https://www.cnblogs.com/guoyaohua/p/8542554.html

http://fyubang.com/2019/08/10/optimizer_sgd/
     
